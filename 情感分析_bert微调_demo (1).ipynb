{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调 Chinese-BERT-wwm-ext 模型用于三分类情感分析\n",
    "\n",
    "本 Notebook 用于在魔塔平台上微调 `hfl/chinese-bert-wwm-ext` 模型，\n",
    "数据源为本地上传的 `评论情感分析结果_三分类_人工纠错.xlsx` 文件，使用 Huggingface `transformers` 库进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T06:26:52.763934Z",
     "iopub.status.busy": "2025-05-13T06:26:52.763651Z",
     "iopub.status.idle": "2025-05-13T06:26:57.870768Z",
     "shell.execute_reply": "2025-05-13T06:26:57.870271Z",
     "shell.execute_reply.started": "2025-05-13T06:26:52.763920Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/site-packages (4.48.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 安装所需库\n",
    "!pip install transformers datasets scikit-learn openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T06:26:59.904201Z",
     "iopub.status.busy": "2025-05-13T06:26:59.903891Z",
     "iopub.status.idle": "2025-05-13T06:27:08.198482Z",
     "shell.execute_reply": "2025-05-13T06:27:08.198024Z",
     "shell.execute_reply.started": "2025-05-13T06:26:59.904178Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 14:27:05.567220: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-13 14:27:05.607916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-13 14:27:06.788014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-13T06:27:15.999048Z",
     "iopub.status.busy": "2025-05-13T06:27:15.998506Z",
     "iopub.status.idle": "2025-05-13T06:27:16.044195Z",
     "shell.execute_reply": "2025-05-13T06:27:16.043598Z",
     "shell.execute_reply.started": "2025-05-13T06:27:15.999026Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'gender', 'level', 'comment', 'likes', 'emotion', 'score',\n",
      "       'status'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 正确读取 CSV 文件\n",
    "df = pd.read_csv(\"评论情感分析结果_三分类_人工纠错.csv\")\n",
    "print(df.columns)\n",
    "\n",
    "# 只保留需要的列\n",
    "df = df[[\"comment\", \"emotion\"]].dropna()\n",
    "\n",
    "# 标签编码\n",
    "label2id = {label: i for i, label in enumerate(sorted(df[\"emotion\"].unique()))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "df[\"label\"] = df[\"emotion\"].map(label2id)\n",
    "\n",
    "# 划分训练和验证集\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "# 转换为 Huggingface Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"comment\", \"label\"]])\n",
    "val_dataset = Dataset.from_pandas(val_df[[\"comment\", \"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-13T06:43:20.730499Z",
     "iopub.status.busy": "2025-05-13T06:43:20.730021Z",
     "iopub.status.idle": "2025-05-13T06:49:33.039234Z",
     "shell.execute_reply": "2025-05-13T06:49:33.038683Z",
     "shell.execute_reply.started": "2025-05-13T06:43:20.730479Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9342b568a23440bf93df158147174f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4752caef66e484baa1caf20ad6f3055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./modelbert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 06:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.840400</td>\n",
       "      <td>0.776739</td>\n",
       "      <td>0.621993</td>\n",
       "      <td>0.491917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.713500</td>\n",
       "      <td>0.670688</td>\n",
       "      <td>0.625430</td>\n",
       "      <td>0.484861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.610800</td>\n",
       "      <td>0.573873</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>0.726990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.500900</td>\n",
       "      <td>0.474335</td>\n",
       "      <td>0.788660</td>\n",
       "      <td>0.789913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>0.448646</td>\n",
       "      <td>0.798969</td>\n",
       "      <td>0.792224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.542722</td>\n",
       "      <td>0.807560</td>\n",
       "      <td>0.795235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.500573</td>\n",
       "      <td>0.807560</td>\n",
       "      <td>0.802721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.156800</td>\n",
       "      <td>0.623886</td>\n",
       "      <td>0.780069</td>\n",
       "      <td>0.774986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.925155</td>\n",
       "      <td>0.774914</td>\n",
       "      <td>0.767471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.838126</td>\n",
       "      <td>0.774914</td>\n",
       "      <td>0.767471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练完成，正在评估最终模型...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终评估结果: {'eval_loss': 0.5005733370780945, 'eval_accuracy': 0.8075601374570447, 'eval_f1': 0.8027207865194665, 'eval_runtime': 2.6099, 'eval_samples_per_second': 222.996, 'eval_steps_per_second': 3.832, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final_model/tokenizer_config.json',\n",
       " './final_model/special_tokens_map.json',\n",
       " './final_model/vocab.txt',\n",
       " './final_model/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义本地模型路径\n",
    "model_path = \"./modelbert\" # 假设你的模型文件在当前目录下的 modelbert 文件夹中\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 定义 tokenization 函数\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"comment\"], padding=\"max_length\", truncation=True, max_length=256) # 你可以根据需要调整 max_length\n",
    "\n",
    "# 应用 tokenization 到数据集\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 加载模型\n",
    "# num_labels 需要设置为你的分类类别数量 (这里是 3)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id)\n",
    "\n",
    "# 定义评估指标\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted') # 对于多分类使用 weighted f1\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",             # 训练输出目录\n",
    "    num_train_epochs=10,                 # 训练的总 epoch 数，可以根据需要调整\n",
    "    per_device_train_batch_size=64,     # 每个设备的训练 batch size\n",
    "    per_device_eval_batch_size=64,      # 每个设备的评估 batch size\n",
    "    warmup_steps=500,                   # warmup 步数\n",
    "    learning_rate=3e-5,                 # 学习率\n",
    "    weight_decay=0.01,                  # weight decay\n",
    "    logging_dir=\"./logs\",               # log 目录\n",
    "    logging_steps=10,                   # 每隔多少步记录一次 log\n",
    "    evaluation_strategy=\"epoch\",        # 每个 epoch 结束后进行评估\n",
    "    save_strategy=\"epoch\",              # 每个 epoch 结束后保存模型\n",
    "    load_best_model_at_end=True,        # 训练结束后加载在验证集上表现最好的模型\n",
    "    metric_for_best_model=\"f1\",         # 用 f1 作为选择最好模型的指标\n",
    "    greater_is_better=True,             # f1 越大越好\n",
    "    report_to=\"none\",                   # 不上传到任何平台，如果需要可以改为 \"tensorboard\" 或其他\n",
    ")\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # 要训练的模型\n",
    "    args=training_args,                  # 训练参数\n",
    "    train_dataset=tokenized_train_dataset, # 训练数据集\n",
    "    eval_dataset=tokenized_val_dataset,  # 评估数据集\n",
    "    compute_metrics=compute_metrics,     # 评估指标函数\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "print(\"开始训练...\")\n",
    "trainer.train()\n",
    "\n",
    "# 评估训练完成后的模型（加载的是最好的模型）\n",
    "print(\"训练完成，正在评估最终模型...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"最终评估结果: {eval_results}\")\n",
    "\n",
    "# 你现在可以保存最终的模型和 tokenizer\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
