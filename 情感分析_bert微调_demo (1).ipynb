{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¾®è°ƒ Chinese-BERT-wwm-ext æ¨¡å‹ç”¨äºä¸‰åˆ†ç±»æƒ…æ„Ÿåˆ†æ\n",
    "\n",
    "æœ¬ Notebook ç”¨äºåœ¨é­”å¡”å¹³å°ä¸Šå¾®è°ƒ `hfl/chinese-bert-wwm-ext` æ¨¡å‹ï¼Œ\n",
    "æ•°æ®æºä¸ºæœ¬åœ°ä¸Šä¼ çš„ `è¯„è®ºæƒ…æ„Ÿåˆ†æç»“æœ_ä¸‰åˆ†ç±»_äººå·¥çº é”™.xlsx` æ–‡ä»¶ï¼Œä½¿ç”¨ Huggingface `transformers` åº“è¿›è¡Œè®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T06:26:52.763934Z",
     "iopub.status.busy": "2025-05-13T06:26:52.763651Z",
     "iopub.status.idle": "2025-05-13T06:26:57.870768Z",
     "shell.execute_reply": "2025-05-13T06:26:57.870271Z",
     "shell.execute_reply.started": "2025-05-13T06:26:52.763920Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/site-packages (4.48.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£…æ‰€éœ€åº“\n",
    "!pip install transformers datasets scikit-learn openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T06:26:59.904201Z",
     "iopub.status.busy": "2025-05-13T06:26:59.903891Z",
     "iopub.status.idle": "2025-05-13T06:27:08.198482Z",
     "shell.execute_reply": "2025-05-13T06:27:08.198024Z",
     "shell.execute_reply.started": "2025-05-13T06:26:59.904178Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 14:27:05.567220: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-13 14:27:05.607916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-13 14:27:06.788014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-13T06:27:15.999048Z",
     "iopub.status.busy": "2025-05-13T06:27:15.998506Z",
     "iopub.status.idle": "2025-05-13T06:27:16.044195Z",
     "shell.execute_reply": "2025-05-13T06:27:16.043598Z",
     "shell.execute_reply.started": "2025-05-13T06:27:15.999026Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'gender', 'level', 'comment', 'likes', 'emotion', 'score',\n",
      "       'status'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# æ­£ç¡®è¯»å– CSV æ–‡ä»¶\n",
    "df = pd.read_csv(\"è¯„è®ºæƒ…æ„Ÿåˆ†æç»“æœ_ä¸‰åˆ†ç±»_äººå·¥çº é”™.csv\")\n",
    "print(df.columns)\n",
    "\n",
    "# åªä¿ç•™éœ€è¦çš„åˆ—\n",
    "df = df[[\"comment\", \"emotion\"]].dropna()\n",
    "\n",
    "# æ ‡ç­¾ç¼–ç \n",
    "label2id = {label: i for i, label in enumerate(sorted(df[\"emotion\"].unique()))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "df[\"label\"] = df[\"emotion\"].map(label2id)\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "# è½¬æ¢ä¸º Huggingface Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"comment\", \"label\"]])\n",
    "val_dataset = Dataset.from_pandas(val_df[[\"comment\", \"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-05-13T06:43:20.730499Z",
     "iopub.status.busy": "2025-05-13T06:43:20.730021Z",
     "iopub.status.idle": "2025-05-13T06:49:33.039234Z",
     "shell.execute_reply": "2025-05-13T06:49:33.038683Z",
     "shell.execute_reply.started": "2025-05-13T06:43:20.730479Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9342b568a23440bf93df158147174f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4752caef66e484baa1caf20ad6f3055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./modelbert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹è®­ç»ƒ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='370' max='370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [370/370 06:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.840400</td>\n",
       "      <td>0.776739</td>\n",
       "      <td>0.621993</td>\n",
       "      <td>0.491917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.713500</td>\n",
       "      <td>0.670688</td>\n",
       "      <td>0.625430</td>\n",
       "      <td>0.484861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.610800</td>\n",
       "      <td>0.573873</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>0.726990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.500900</td>\n",
       "      <td>0.474335</td>\n",
       "      <td>0.788660</td>\n",
       "      <td>0.789913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>0.448646</td>\n",
       "      <td>0.798969</td>\n",
       "      <td>0.792224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.542722</td>\n",
       "      <td>0.807560</td>\n",
       "      <td>0.795235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.500573</td>\n",
       "      <td>0.807560</td>\n",
       "      <td>0.802721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.156800</td>\n",
       "      <td>0.623886</td>\n",
       "      <td>0.780069</td>\n",
       "      <td>0.774986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.925155</td>\n",
       "      <td>0.774914</td>\n",
       "      <td>0.767471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.838126</td>\n",
       "      <td>0.774914</td>\n",
       "      <td>0.767471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨è¯„ä¼°æœ€ç»ˆæ¨¡å‹...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€ç»ˆè¯„ä¼°ç»“æœ: {'eval_loss': 0.5005733370780945, 'eval_accuracy': 0.8075601374570447, 'eval_f1': 0.8027207865194665, 'eval_runtime': 2.6099, 'eval_samples_per_second': 222.996, 'eval_steps_per_second': 3.832, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final_model/tokenizer_config.json',\n",
       " './final_model/special_tokens_map.json',\n",
       " './final_model/vocab.txt',\n",
       " './final_model/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å®šä¹‰æœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "model_path = \"./modelbert\" # å‡è®¾ä½ çš„æ¨¡å‹æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹çš„ modelbert æ–‡ä»¶å¤¹ä¸­\n",
    "\n",
    "# åŠ è½½ tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# å®šä¹‰ tokenization å‡½æ•°\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"comment\"], padding=\"max_length\", truncation=True, max_length=256) # ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ max_length\n",
    "\n",
    "# åº”ç”¨ tokenization åˆ°æ•°æ®é›†\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "# num_labels éœ€è¦è®¾ç½®ä¸ºä½ çš„åˆ†ç±»ç±»åˆ«æ•°é‡ (è¿™é‡Œæ˜¯ 3)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id)\n",
    "\n",
    "# å®šä¹‰è¯„ä¼°æŒ‡æ ‡\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted') # å¯¹äºå¤šåˆ†ç±»ä½¿ç”¨ weighted f1\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "# å®šä¹‰è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",             # è®­ç»ƒè¾“å‡ºç›®å½•\n",
    "    num_train_epochs=10,                 # è®­ç»ƒçš„æ€» epoch æ•°ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "    per_device_train_batch_size=64,     # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒ batch size\n",
    "    per_device_eval_batch_size=64,      # æ¯ä¸ªè®¾å¤‡çš„è¯„ä¼° batch size\n",
    "    warmup_steps=500,                   # warmup æ­¥æ•°\n",
    "    learning_rate=3e-5,                 # å­¦ä¹ ç‡\n",
    "    weight_decay=0.01,                  # weight decay\n",
    "    logging_dir=\"./logs\",               # log ç›®å½•\n",
    "    logging_steps=10,                   # æ¯éš”å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡ log\n",
    "    evaluation_strategy=\"epoch\",        # æ¯ä¸ª epoch ç»“æŸåè¿›è¡Œè¯„ä¼°\n",
    "    save_strategy=\"epoch\",              # æ¯ä¸ª epoch ç»“æŸåä¿å­˜æ¨¡å‹\n",
    "    load_best_model_at_end=True,        # è®­ç»ƒç»“æŸååŠ è½½åœ¨éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹\n",
    "    metric_for_best_model=\"f1\",         # ç”¨ f1 ä½œä¸ºé€‰æ‹©æœ€å¥½æ¨¡å‹çš„æŒ‡æ ‡\n",
    "    greater_is_better=True,             # f1 è¶Šå¤§è¶Šå¥½\n",
    "    report_to=\"none\",                   # ä¸ä¸Šä¼ åˆ°ä»»ä½•å¹³å°ï¼Œå¦‚æœéœ€è¦å¯ä»¥æ”¹ä¸º \"tensorboard\" æˆ–å…¶ä»–\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ– Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # è¦è®­ç»ƒçš„æ¨¡å‹\n",
    "    args=training_args,                  # è®­ç»ƒå‚æ•°\n",
    "    train_dataset=tokenized_train_dataset, # è®­ç»ƒæ•°æ®é›†\n",
    "    eval_dataset=tokenized_val_dataset,  # è¯„ä¼°æ•°æ®é›†\n",
    "    compute_metrics=compute_metrics,     # è¯„ä¼°æŒ‡æ ‡å‡½æ•°\n",
    ")\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "trainer.train()\n",
    "\n",
    "# è¯„ä¼°è®­ç»ƒå®Œæˆåçš„æ¨¡å‹ï¼ˆåŠ è½½çš„æ˜¯æœ€å¥½çš„æ¨¡å‹ï¼‰\n",
    "print(\"è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨è¯„ä¼°æœ€ç»ˆæ¨¡å‹...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"æœ€ç»ˆè¯„ä¼°ç»“æœ: {eval_results}\")\n",
    "\n",
    "# ä½ ç°åœ¨å¯ä»¥ä¿å­˜æœ€ç»ˆçš„æ¨¡å‹å’Œ tokenizer\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
